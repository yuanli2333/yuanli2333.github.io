<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="favicon.ico" />
<link rel="bookmark" href="favicon.ico" type="image/x-icon"　/>
<title>YUAN, Li (袁粒)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="https://scholar.google.com/citations?user=-5juAR0AAAAJ&hl=en">Google Scholar</a></div>
<div class="menu-item"><a href="https://github.com/PKU-YuanGroup">GitHub of Lab</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>YUAN, Li (袁粒) </h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://yuanli2333.github.io/"><img src="photos/Li.png" alt="alt text" width="131px" height="160px" /></a>&nbsp;</td>
<td align="left"><p>Tenure-track Assistant Professor<br /></p>
  <a href="https://www.ece.pku.edu.cn/"> School of Electrical and Computer Engineering (ece.pku.edu.cn), </a>  <br />
  <a href="https://english.pkusz.edu.cn/"> Shenzhen Graduate School, </a><br />
  <a href=https://www.pku.edu.cn/department.html> Peking University</a> <br /></p>
  <a href="https://www.ece.pku.edu.cn/info/1046/2678.htm">Official Profile (Chinese)</p>
  </a> Email: <a href = "mailto: yuanli-ece@pku.edu.cn">yuanli-ece@pku.edu.cn</a>
</td></tr></table>

<h2><font color="#FF0000"><strong>Job Openings</strong></font></h2>
<ul>
<li><p>Postdoctoral research fellow positions are available (常年招收北大博雅博后).
</li>
<li><p>I am recruiting PhD students and Master students in Research (MPhil). 
</li>
<li><p>Our group also seek geeky students don't need papers: those with strong AI Agent, Vibe Coding, and full-stack engineering skills(AI Agent、Vibe Coding、全栈等极客类学生无论文要求). 
</ul>

<h2>Education</h2>
<ul>
<li><p>Ph.D. <a href="https://www.nus.edu.sg/ ">National University of Singapore, NUS</a>, 2017.08-2021.10</p>
   <p>(Visiting PhD Student at <a href="https://livingstone.hms.harvard.edu/people">Harvard University</a>, 2019.09-2020.06)</p>
</li>
<li><p>B.E. <a href=" https://en.ustc.edu.cn/"> University of Science and Technology of China, USTC </a>, 2013.09-2017.06</p>
</ul>
</p> In NUS, I did research in <a href="https://www.ece.nus.edu.sg/lv/people.html"> Learning and Vision lab </a>, fortunately advised by <a href="https://sites.google.com/site/jshfeng/home"> Prof. Jiashi Feng</a>, Prof. Francis EH Tay and <a href=" https://yanshuicheng.ai/ "> Prof. Shuicheng Yan</a>, and I also spend more than half year in Harvard University, fortunately advised by <a href="https://neuro.hms.harvard.edu/faculty-staff/margaret-livingstone"> Prof. Margaret Livingstone. </a> </p>

<h2>Work Experiences</h2>
<ul>
<li><p> Peking University, School of ECE, Shenzhen Graduate School, Assistant Professor, 2022.01-now
  </li>
<li><p>YITU Singapore, PhD research intern, work with Dr.Yunpeng Chen and Prof.Shuicheng Yan, 2020.06-2021.01
</li>
<li><p>Tencent AI Lab,  PhD research intern, work with Dr.Wei Liu and Dr.Zequn Jie, 2018.12-2019.03
</li>
</ul>

<h2>Open-Source Project in my Lab</h2>
<ul>
  <li><p> <a href="https://github.com/PKU-YuanGroup">All open-source projects in my lab:</a> https://github.com/PKU-YuanGroup
  </li>
  <li><p> <a href="https://github.com/PKU-YuanGroup/Open-Sora-Plan">Open Sora Plan</a>
  </li>
<li><p> <a href="https://www.chatexcel.com">ChatExcel</a>
  </li>
<li><p> <a href="https://chatlaw.cloud/">ChatLaw</a>
  </li>
</ul>

<h2>Research</h2>
<p>My research interests include: </p>
<ul>
<li><p>Computer Vision</p>
</li>
<li><p>Multi-Modal Machine Learning</p>
</li>
<li><p>AI for Science</p>
</li>
</ul>

<p><a href="https://scholar.google.com/citations?user=-5juAR0AAAAJ&hl=en">Full list of publications in Google Scholar</a>.</p>

<h3>
	<a name='publications'></a> Selected Publications  
</h3>
</p> † corresponding author
</p>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Deep peak property learning for efficient chiral molecules ECD spectra prediction
      </strong><br /> Hao Li, Da Long, <strong>Li Yuan†</strong>,  Yonghong Tian, Xinchang Wang†, Fanyang Mo†. <br />
       <strong>Nature Computational Science</strong>, 2025.  <br />
      <a href="https://arxiv.org/abs/2401.03403">[AI for Science]</a>
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>BASALT refines binning from metagenomic data and increases resolution of genome-resolved metagenomic analysis
      </strong><br /> Zhiguang Qiu, <strong>Li Yuan</strong>, Chun-Ang Lian, Bin Lin, Jie Chen, Rong Mu, Xuejiao Qiao, Liyu Zhang, Zheng Xu, Lu Fan, Yunzeng Zhang, Shanquan Wang, Junyi Li, Huiluo Cao, Bing Li, Yong-Xin Liu, Lili Shi, Yonghong Tian, Jinren Ni, Jizhong Zhou, Weiqin Zhuang, Ke Yu. <br />
       <strong>Nature Communications</strong>, 2024.  <br />
      <a href="https://www.nature.com/articles/s41467-024-46539-7">[AI for Science]</a>
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation
      </strong><br />
      Munan Ning, Donghuan Lu, Yujia Xie, Dongdong Chen, Dong Wei, Yefeng Zheng,
      Yonghong Tian, Shuicheng Yan, <strong>Li Yuan†</strong> <br />
       IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2023. <br />
      <a href="https://arxiv.org/abs/2301.07354">[Arxiv]</a>
	    <a href="https://github.com/munanning/MADA">[Code]</a> <br />
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>DiffusionRet: Generative Text-Video Retrieval with Diffusion Model
      </strong><br />
      Peng Jin, Hao Li, Zesen Cheng, Kehan Li, Xiangyang Ji, Chang Liu, <strong>Li Yuan†</strong>, Jie Chen† <br />
      International Conference on Computer Vision (<strong>ICCV</strong>), 2023. Accepted, to be appear <br />
      <a href="https://arxiv.org/abs/2303.09867">[Arxiv]</a>
	    <a href="https://github.com/jpthu17/DiffusionRet">[Code]</a> <br />
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Rethinking Point Cloud Registration as Masking and Reconstruction
      </strong><br />
      Guangyan Chen, Meiling Wang, <strong>Li Yuan†</strong>, Yi Yang, Yufeng Yue† <br />
      International Conference on Computer Vision (<strong>ICCV</strong>), 2023. Accepted, to be appear <br />
      <a href="">[Arxiv]</a>
	    <a href="">[Code]</a> <br />
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Multi-granularity Interaction Simulation for Unsupervised Interactive Segmentation
      </strong><br />
      Kehan Li, Yian Zhao, Zhennan Wang, Zesen Cheng, Peng Jin, Xiangyang Ji, Li Yuan, Chang Liu, Jie Chen <br />
      International Conference on Computer Vision (<strong>ICCV</strong>), 2023. Accepted, to be appear <br />
      <a href="https://arxiv.org/abs/2303.13399">[Arxiv]</a>
	    <a href="">[Code]</a> <br />
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning
      </strong><br />
       Peng Jin, JinFa Huang, Pengfei Xiong, Shangxuan Tian, Chang Liu, Xiangyang Ji, <strong>Li Yuan†</strong>, Jie Chen†<br /> 
      IEEE Computer Vision and Pattern Recognition (<strong>CVPR</strong>), <strong>Highlight</strong> (Top 10%), 2023. <br />
      <a href="https://arxiv.org/abs/2303.14369">[Arxiv]</a>
	    <a href="https://jpthu17.github.io/HBI/">[Code]</a> <br />
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>ACSeg: Adaptive Conceptualization for Unsupervised Semantic Segmentation
      </strong><br />
      Kehan Li, Zhennan Wang, Zesen Cheng, Runyi Yu, Yian Zhao, Guoli Song, <strong>Li Yuan†</strong>, Jie Chen†<br /> 
      IEEE Computer Vision and Pattern Recognition (<strong>CVPR</strong>), <strong>Highlight</strong> (Top 10%), 2023. <br />
      <a href="https://arxiv.org/abs/2210.05944">[Arxiv]</a>
	    <a href="coming soon">[Code]</a> <br />
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Learning with Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning
      </strong><br />
      Zeyin Song, Yifan Zhao, Yujun Shi, Peixi Peng, <strong>Li Yuan</strong>, Yonghong Tian<br /> 
      IEEE Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023. <br />
      <a href="https://arxiv.org/abs/2304.00426">[Arxiv]</a>
	    <a href="https://github.com/zysong0113/SAVC">[Code]</a> <br />
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Out-of-Candidate Rectification for Weakly Supervised Semantic Segmentation
      </strong><br />
      Zesen Cheng, Pengchong Qiao, Kehan Li, Siheng Li, Pengxu Wei, Xiangyang Ji, <strong>Li Yuan</strong>, Chang Liu, Jie Chen <br /> 
      IEEE Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023. <br />
      <a href="https://arxiv.org/abs/2211.12268">[Arxiv]</a>
	    <a href="https://github.com/sennnnn/Out-of-Candidate-Rectification">[Code]</a> <br />
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Deep Interactive Full Transformer Framework for Point Cloud Registration
      </strong><br />
      Guangyan Chen, Meiling Wang, Yufeng Yue, Qingxiang Zhang,<strong>Li Yuan</strong><br /> 
      IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>), 2023. <br />
      <a href="https://arxiv.org/abs/2112.09385">[Arxiv]</a>
	    <a href="https://github.com/CGuangyan-BIT/DIT">[Code]</a> <br />
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Spikformer: When Spiking Neural Network Meets Transformer
      </strong><br />
      Zhaokun Zhou, Yuesheng Zhu, Chao He, Yaowei Wang, Shuicheng Yan, Yonghong Tian,<strong>Li Yuan†</strong><br /> 
      International Conference on Learning Representations (<strong>ICLR</strong>), 2023. <br />
      <a href="https://arxiv.org/abs/2209.15425">[Arxiv]</a>
	    <a href="https://github.com/ZK-Zhou/spikformer">[Code]</a> <br />
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>VOLO: Vision Outlooker for Visual Recognition
      </strong><br />
       <strong>Li Yuan</strong>, Qibin Hou, Zihang Jiang, Jiashi Feng, Shuicheng Yan<br /> 
       IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2022. <br />
       VOLO is the first model exceeds 87.0% top-1 accuracy on ImageNet without using extra data <br />
      <a href="https://arxiv.org/abs/2106.13112">[Arxiv]</a>
	    <a href="https://github.com/sail-sg/volo">[Code]</a> <br />
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition     </strong><br />
      Qibin Hou, Zihang Jiang, <strong>Li Yuan</strong>, Ming-Ming Cheng, Shuicheng Yan, Jiashi Feng<br /> 
      IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2022 <br />
      <a href="https://arxiv.org/abs/2106.12368">[Arxiv]</a>
	    <a href="https://github.com/Andrew-Qibin/VisionPermutator">[Code]</a> <br />
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Masked Autoencoders for Point Cloud Self-supervised Learning
      </strong><br />
      Yatian Pang, Wenxiao Wang, Francis E.H. Tay, Wei Liu, Yonghong Tian, <strong>Li Yuan†</strong> <br /> 
      European Conference on Computer Vision (<strong>ECCV</strong>), 2022 <br />
      <a href="https://arxiv.org/abs/2203.06604">[Arxiv]</a>
	    <a href="https://github.com/Pang-Yatian/Point-MAE">[Code]</a> <br />
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Improving Vision Transformers by Revisiting High-frequency Components
      </strong><br />
      Jiawang Bai, <strong>Li Yuan†</strong>, Shu-Tao Xia†, Shuicheng Yan, Zhifeng Li, Wei Liu†<br /> 
      European Conference on Computer Vision (<strong>ECCV</strong>), 2022 <br />
      <a href="https://arxiv.org/abs/2204.00993">[Arxiv]</a>
	    <a href="https://github.com/jiawangbai/HAT">[Code]</a> <br />
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Locality Guidance for Improving Vision Transformers on Tiny Datasets
      </strong><br />
      Kehan Li, Runyi Yu, Zhennan Wang, <strong>Li Yuan†</strong>, Guoli Song, Jie Chen†<br /> 
      European Conference on Computer Vision (<strong>ECCV</strong>), 2022 <br />
      <a href="https://arxiv.org/abs/2207.10026">[Arxiv]</a>
	    <a href="https://github.com/lkhl/tiny-transformers">[Code]</a> <br />
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet
      </strong><br />
       <strong>Li Yuan</strong>,  Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, Shuicheng Yan<br /> 
       International Conference on Computer Vision (<strong>ICCV</strong>), 2021 <br />
	    <a href="https://arxiv.org/abs/2101.11986">[Arxiv]</a>
	    <a href="https://github.com/yitu-opensource/T2T-ViT">[Code]</a>
   </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>PnP-DETR: Towards Efficient Visual Analysis with Transformers
      </strong><br />
       Tao Wang, <strong>Li Yuan</strong>, Yunpeng Chen, Jiashi Feng, Shuicheng Yan<br /> 
       International Conference on Computer Vision (<strong>ICCV</strong>), 2021 <br />
	    <a href="https://arxiv.org/abs/2109.07036">[Arxiv]</a>
	    <a href="https://github.com/twangnh/pnp-detr">[Code]</a>
   </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Continual Learning via Bit-Level Information Preserving
      </strong><br />
       Yujun Shi, <strong>Li Yuan</strong>, Yunpeng Chen, Jiashi Feng<br /> 
       IEEE Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2021 <br />
	    <a href="https://arxiv.org/abs/2105.04444">[Arxiv]</a>
	    <a href="https://github.com/Yujun-Shi/BLIP">[Code]</a>
   </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Revisiting Knowledge Distillation via Label Smoothing Regularization</strong><br />
       <strong>Li Yuan</strong>, Francis EH Tay, Guilin Li, Tao Wang, Jiashi Feng<br /> 
       IEEE Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2020 (<font color="#FF0000"><strong>Oral</strong></font>) <br />
	    <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.html">[Arxiv]</a>
	    <a href="https://github.com/yuanli2333/Teacher-free-Knowledge-Distillation">[Code]</a>
   </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Central Similarity Quantization for Efficient Image and Video Retrieval</strong><br />
       <strong>Li Yuan</strong>, Tao Wang, Xiaopeng Zhang, Francis EH Tay, Zequn Jie, Wei Liu, Jiashi Feng<br /> 
       IEEE Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2020 <br />
	    <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Yuan_Central_Similarity_Quantization_for_Efficient_Image_and_Video_Retrieval_CVPR_2020_paper.html">[Arxiv]</a>
	    <a href="https://github.com/yuanli2333/Hadamard-Matrix-for-hashing">[Code]</a>
   </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Toward Accurate Person-level Action Recognition in Videos of Crowded Scenes</strong><br />
       <strong>Li Yuan</strong>, Yichen Zhou, Shuning Chang, Ziyuan Huang, Yupeng Chen, Xuecheng Nie, Tao Wang, Jiashi Feng, Shuicheng Yan<br /> 
       ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2020 <br />
   </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Cycle-SUM: Cycle-Consistent Adversarial LSTM Networks for Unsupervised Video Summarization      </strong><br />
      <strong>Li Yuan</strong>, Francis Eng Hock Tay, Ping Li,  Li Zhou, Jiashi Feng<br />
       AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2019 <br />
   </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Unsupervised Video Summarization With Cycle-Consistent Adversarial LSTM Networks</strong><br />
       <strong>Li Yuan</strong>, Francis Eng Hock Tay, Ping Li, Jiashi Feng<br /> 
       IEEE Transactions on Multimedia  (<strong>IEEE TMM</strong>), 2019 <br />
   </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>DynaMixer: A Vision MLP Architecture with Dynamic Mixing  </strong><br />
      Ziyu Wang, Wenhao Jiang, Yiming Zhu, <strong>Li Yuan</strong>, Yibing Song, Wei Liu<br />
      International Conference on Machine Learning (<strong>ICML</strong>), 2022<br />
      <a href="https://arxiv.org/abs/2201.12083"> [Arxiv]</a>
    </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Positive-Negative Momentum: Manipulating Stochastic Gradient Noise to Improve Generalization</strong><br />
       Zeke Xie, <strong>Li Yuan</strong>, Zhanxing Zhu, Masashi Sugiyama<br /> 
       International Conference on Machine Learning (<strong>ICML</strong>), 2021 (<font color="#FF0000"><strong>Spotlight</strong></font>) <br />
	    <a href="https://arxiv.org/abs/2103.17182">[Arxiv]</a>
	    <a href="https://github.com/zeke-xie/Positive-Negative-Momentum">[Code]</a>
   </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>All Tokens Matter: Token Labeling for Training Better Vision Transformers</strong><br />
      Zihang Jiang, Qibin Hou, <strong>Li Yuan</strong>,  Daquan Zhou, Yujun Shi, Xiaojie Jin, Anran Wang, Jiashi Feng<br /> 
      Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2021 <br />
	    <a href="https://arxiv.org/abs/2104.10858">[Arxiv]</a>
	    <a href="https://github.com/zihangJiang/TokenLabeling">[Code]</a>
   </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Distilling Object Detectors With Fine-Grained Feature Imitation</strong><br />
      Tao Wang, <strong>Li Yuan</strong>,  Xiaopeng Zhang, Jiashi Feng<br /> 
      IEEE Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2019 <br />
	    <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Distilling_Object_Detectors_With_Fine-Grained_Feature_Imitation_CVPR_2019_paper.html">[Arxiv]</a>
	    <a href="https://github.com/twangnh/Distilling-Object-Detectors">[Code]</a>
   </p>
  </div>
</div>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Few-shot adaptive faster r-cnn</strong><br />
      Tao Wang, Xiaopeng Zhang, <strong>Li Yuan</strong>, Jiashi Feng<br /> 
      IEEE Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2019 <br />
   </p>
  </div>
</div>

<h3>
	<a name='publications'></a> Preprint
</h3>

<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong>Adversarial Images for the Primate Brain
      </strong><br />
       <strong>Li Yuan</strong>, Will Xiao, Gabriel Kreiman, Francis E.H. Tay, Jiashi Feng, Margaret S. Livingstone<br /> 
      <a href="https://arxiv.org/abs/2011.05623">[Arxiv]</a>
	    <a href="">[Code Coming]</a> <br />
    </p>
  </div>
</div>

<h3>Academic service</h3>
<p><b>Guest Editor</b></p>
<ul>
<li><p>MDPI-Sensors (IF: 3.57)</p>
</li>
</ul>
<p><b>Associate Editor</b></p>
<ul>
<li><p>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) </p>
</li>
</ul>

<p><b>(Senior) Program Committee or Reviewer</b></p>
<ul>
<li><p>CVPR, ICCV, ECCV, ICML, NeurIPS, ICLR, AAAI, CoLLAs etc.</p>
</li>
<li><p>IEEE TPAMI, TNNLS, TMM, TCSVT , Neurocomputing, Pattern Recognition etc.</p>
</li>
</ul>

<p><b>Awards</b></p>
<ul>
<li><p> <a href="https://www.forbes.com/profile/yuan-li-1/?sh=5d00848c5ba4">Forbes Asia 30 Under 30, 2023</a>  </p>
</li>
<li><p>Chinese Government Award for Outstanding Students Abroad(50 scholarships in 2021)</p>
</li>
<li><p>NUS Research Scholarship (2017-2021)</p>
</li>
<li><p> <a href="https://2020.acmmm.org/awards.html">ACM MM Grand Challenge Winner, 2020.10</a> </p>
</li>
<li><p>Winner (1'st Place) of "Human in Event" Video Understanding Challenge, Track4 (2020.06)</p>
</li>
</ul>

</td>
</tr>
</table>
</body>
</html>
